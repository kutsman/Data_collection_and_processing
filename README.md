# Data_collection_and_processing

### УРОК 4. MongoDB

1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, которая будет добавлять только новые вакансии/продукты в вашу базу.

Разворачивал MongoDB в Colab'е. С тестированием записи в коллекцию.
Далее добавил в функцию для парсинга вакансий с сайта HH опцию поиска и сравнения спарсенной вакансии в коллекции, если ссылка на вакансию уже есть, то идет обновление ее данных, если ссылки в коллекции нет, то полностью добавляется вакансия.

2. Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введённой суммы (необходимо анализировать оба поля зарплаты).

Добавил функцию поиска вакансий с требуемой ЗП и валютой. Выводит все вакансии с указаной валютой и уровнем оклада (мин и макс) больше указаной в запросе.

### Урок 5.

Вариант II

2) Написать программу, которая собирает товары «В тренде» с сайта техники mvideo и складывает данные в БД. Сайт можно выбрать и свой. Главный критерий выбора: динамически загружаемые товары
Если mvideo не доступен (что иногда бывает), можете использовать любой сайт с лентой специальных предложений, к примеру: https://www.eldorado.ru/, https://5ka.ru/, https://www.carrefoursa.com/ и т.п.

Так как объект не должен быть статическим.

Брал новости с сайта lenta.ru, парсил все новости на текущей странице (парсинг с проваливанием по ссылке новости и затягивание заголовка, ссылки, даты и времени)

Далее паук определяет следующую страницу и парсит аналогично. Выставлено условие, при достижение определенной страницы (глубину можно выставить любую), паук останавливается (чтобы не парсил много новостей).

Проверяется наличие новости в базе mongodb, чтобы не дублировались новости.

### Урок 6. Фреймворк Scrapy, pipelines, Splash 
I вариант
1) Доработать паука в имеющемся проекте, чтобы он формировал item по структуре:
*Наименование вакансии
*Зарплата от
*Зарплата до
*Ссылку на саму вакансию
И складывал все записи в БД(любую)

Разворачивал MongoDB в Colab'е. С тестированием записи в коллекцию.

Далее добавил в функцию для парсинга вакансий с сайта HH и SJ опцию поиска и сравнения спарсенной вакансии в коллекции, если ссылка на вакансию уже есть, то идет обновление ее данных, если ссылки в коллекции нет, то полностью добавляется вакансия.

2) Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. Паук должен формировать item'ы по аналогичной структуре и складывать данные также в БД

Урок 7. Парсинг данных. Selenium в Python
1) Взять любую категорию товаров на сайте Леруа Мерлен. Собрать следующие данные:
● название;
● все фото;
● ссылка;
● цена.

Реализуйте очистку и преобразование данных с помощью ItemLoader. Цены должны быть в виде числового значения.

Взял сайт OBI, так как Леруа постоянно выдает ошибку 403 (пробовал куки, список user-agent, прокси и т.д.)
С использованием ItemLoader (внутри Item обработка цены, ссылки картинки, название файла картинки и создание папки товара и складывание в нее картинок)

Дополнительно:
2)Написать универсальный обработчик характеристик товаров, который будет формировать данные вне зависимости от их типа и количества.

3)Реализовать хранение скачиваемых файлов в отдельных папках, каждая из которых должна соответствовать собираемому товару 
Каждый товар создает свою папку формата: /gazonokosilka-akkumuljatornaja-greenworks-g-max-g40lm35k2-akb-i-zu-3854163
которая берется из ссылки на товар: https://obi.ru/products/gazonokosilka-akkumuljatornaja-greenworks-g-max-g40lm35k2-akb-i-zu-3854163
Далее картинки с названием 3854163_1_a810.jpg складываются в эту папку.
И так для каждого товара.
